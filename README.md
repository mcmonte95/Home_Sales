# Home Sales Analysis with SparkSQL

This challenge involved working with sample data about home sales to extract insights through various queries, leveraging PySpark to handle data processing and analysis tasks. The main goal was to test my knowledge of Pyspark and demonstrate the ability to query the data effectively.

## Challenge Overview

The tasks in this challenge included:
- Reading home sales data into a Spark DataFrame.
- Creating and manipulating temporary tables with SparkSQL.
- Running queries to determine key metrics such as average prices based on specific criteria.
- Utilizing caching to improve query performance.
- Partitioning data to enhance query efficiency.
- Verifying and managing cached data states.

## Note

This challenge was executed in a Google Colab notebook and then downloaded due to the challenge in getting PySpark to work on my local enviroment.

## How to Run

1. Clone this repository to your local machine.
2. Open the `Home_Sales.ipynb` in Google Colab or your Jupyter environment with Spark setup.
